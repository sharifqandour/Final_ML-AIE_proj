# -*- coding: utf-8 -*-
"""Urban_Sound_CNN_sqandour.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11gvW-TLz773Qf74kn20aTBs-xNakFBEw
"""

#import soundata
import os
import pandas as pd
import librosa
#import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import glob
import seaborn as sns
import sklearn
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
import time
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from tensorflow import keras
from tensorflow.keras import layers

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Colab_Notebooks/data

# view the new directory
!pwd

# Commented out IPython magic to ensure Python compatibility.
#change directory
#os.chdir('/Users/sharif/Documents/CodeAcademy/Final_Project_MLAIE/sound_datasets')
os.getcwd()
os.listdir()
# %cd /content/drive/MyDrive/
#os.listdir()

# Commented out IPython magic to ensure Python compatibility.
# Data Pre-processing
# %cd /content/drive/MyDrive/audio
os.listdir()

"""This data has already been initialized -- no need to run again

dataset = soundata.initialize('urbansound8k') dataset.download() # download the dataset -- force_overwrite=True dataset.validate() # validate that all the expected files are there
example_clip = dataset.choice_clip() # choose a random example clip print(example_clip) # see the available data

@misc{fuentes_salamon2021soundata,
      title={Soundata: A Python library for reproducible use of audio datasets},
      author={Magdalena Fuentes and Justin Salamon and Pablo Zinemanas and Martín Rocamora and
      Genís Plaja and Irán R. Román and Marius Miron and Xavier Serra and Juan Pablo Bello},
      year={2021},
      eprint={2109.12690},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
"""

dataset_path = '/content/drive/MyDrive/urbansound8k'
df = pd.read_csv(os.path.join(dataset_path, 'metadata', 'UrbanSound8K.csv'))
for i, row in df.iterrows():
    if i % 999:
        audio_file_path = os.path.join(dataset_path, 'audio', 'fold' + str(row["fold"]), row["slice_file_name"])
        class_label = row["class"]
        print("Processing audio file:", audio_file_path)


        # load the audio file
        y, sr = librosa.load(audio_file_path)

        # extract the mel spectrogram
        S = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)
        log_S = librosa.power_to_db(S, ref=np.max)

        # plot the mel spectrogram
        plt.figure(figsize=(10, 4))
        librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')
        plt.title(class_label)
        plt.colorbar(format='%+2.0f dB')
        plt.tight_layout()
        plt.show()

!pip install --upgrade librosa

df['class'].nunique()

# Load the UrbanSound8K metadata CSV file
metadata = df

# Get summary statistics of the dataset
print(metadata.describe())

# Plot the class distribution of the dataset
plt.figure(figsize=(10, 5))
sns.countplot(metadata['class'])
plt.title('Class Distribution')
plt.show()

# Plot the Mel spectrogram of a random audio file
#filename = '/Users/sharif/Documents/CodeAcademy/Final_Project_MLAIE/sound_datasets/urbansound8k/audio/fold1/7383-3-0-0.wav' #location on HD
filename = '/content/drive/MyDrive/urbansound8k/audio/fold2/204773-3-9-0.wav'
signal, sr = librosa.load(filename, sr=22050)
mel_spec = librosa.feature.melspectrogram(signal, sr=sr, n_mels=128)
log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
plt.figure(figsize=(10, 5))
librosa.display.specshow(log_mel_spec, y_axis='mel', fmax=8000, x_axis='time')
plt.title('Mel Spectrogram of Audio File')
plt.colorbar(format='%+2.0f dB')
#plt.show()
#plt.close()

filepath_temp = '/content/drive/MyDrive/urbansound8k/audio/fold2/204773-3-9-0.wav'
SAMPLE_RATE = 22050
# Load audio file
try:
    signal, sr = librosa.load(filepath_temp, sr=SAMPLE_RATE)
except Exception as e:
    print(f"Error loading file {filepath_temp}: {e}")

signal, sr

dataset_path = '/content/drive/MyDrive/urbansound8k/audio'
#dataset_path = '/content/drive/MyDrive/audio'
SAMPLE_RATE = 22050
DURATION = 4
SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION
NUM_MFCC = 40

#timing
start_time_PreProc = time.time()

def preprocess_dataset(dataset_path):

    data = []
    labels = []

    # Iterate through all subfolders
    for subdir in os.listdir(dataset_path):
        if subdir == '.DS_Store':
            print(f"..end of fold {subdir}")
            continue
        subpath = os.path.join(dataset_path, subdir)
        if not os.path.isdir(subpath):
            print(f" if not {subdir}")
            continue

        print(f"Processing {subdir}...")

        # Iterate through all audio files
        for filename in os.listdir(subpath):
            filepath = os.path.join(subpath, filename)

            # Load audio file
            try:
                signal, sr = librosa.load(filepath, sr=SAMPLE_RATE)
            except Exception as e:
                print(f"Finished processing {subdir}, {filepath}: {e}")
                continue

            # Ensure that all audio clips are of the same length
            if len(signal) >= SAMPLES_PER_TRACK:
                signal = signal[:SAMPLES_PER_TRACK]
            else:
                signal = np.pad(signal, (0, SAMPLES_PER_TRACK - len(signal)), 'constant')

            # Compute MFCCs (Mel-Frequency Cepstral Coefficients)
            mfccs = librosa.feature.mfcc(y=signal, sr=SAMPLE_RATE, n_mfcc=NUM_MFCC)
            mfccs = np.mean(mfccs, axis=1) # take the mean of each feature over time

            # Add to the data and labels lists
            #data.append(mfccs)
            data.append(mfccs.ravel())
            labels.append(subdir)

    # Convert data and labels lists to numpy arrays
    data = np.array(data)
    labels = np.array(labels)

    # Convert labels to integer-encoded class labels
    classes = np.unique(labels)
    label_to_class = dict(zip(classes, range(len(classes))))
    labels = np.array([label_to_class[label] for label in labels])

    # Split the dataset into training and testing sets
    data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2)

    # Reshape data for use in a CNN
    #data_train = data_train.reshape(data_train.shape[0], NUM_MFCC, 1, 1)
    #data_test = data_test.reshape(data_test.shape[0], NUM_MFCC, 1, 1)
      #updating shape(not this shape)
    #data_train = np.reshape(data_train, (data_train.shape[0], NUM_MFCC, int(SAMPLES_PER_TRACK/512), 1))
    #data_test = np.reshape(data_test, (data_test.shape[0], NUM_MFCC, int(SAMPLES_PER_TRACK/512), 1))


    return data_train, data_test, labels_train, labels_test

# Preprocess the dataset
data_train, data_test, labels_train, labels_test = preprocess_dataset(dataset_path)

# Reshape data
#data_train = np.reshape(data_train, (data_train.shape[0], NUM_MFCC, int(SAMPLES_PER_TRACK/512), 1))
#data_test = np.reshape(data_test, (data_test.shape[0], NUM_MFCC, int(SAMPLES_PER_TRACK/512), 1))
#data_train = data_train.reshape(data_train.shape[0], data_train.shape[1])
#data_train = data_train.reshape(data_train.shape[0], NUM_MFCC * DURATION)
#data_test = data_test.reshape(data_test.shape[0], NUM_MFCC * DURATION)
#USE THIS SHAPE
data_train = data_train.reshape(data_train.shape[0], NUM_MFCC, 1, 1)
data_test = data_test.reshape(data_test.shape[0], NUM_MFCC, 1, 1)

#start_time_preproc = time.time()
#data_train, data_test, labels_train, labels_test = preprocess_dataset(DATASET_PATH)
#end_time_preproc = time.time()
#print("Preprocessing time:", end_time_preproc - start_time_preproc, "seconds")


# Print some information about the preprocessed data
print("Number of audio clips:", len(data_train) + len(data_test))
print("Number of training clips:", len(data_train))
print("Number of testing clips:", len(data_test))
print("Number of classes:", len(np.unique(labels_train)))

end_time_PreProc = time.time()
print("Time taken:", end_time_PreProc - start_time_PreProc, "seconds")

print(f'data_train shape: {data_train.shape}')
print(f'data_test shape: {data_test.shape}')
print(f'data_test shape: {labels_train.shape}')
print(f'data_test shape: {labels_test.shape}')

#CNN trial 1 - basic (low accuracy)
input_shape = (40, 1, 1)

labels_train_categorical = keras.utils.to_categorical(labels_train)
labels_test_categorical = keras.utils.to_categorical(labels_test)

model = keras.Sequential()
model.add(layers.Conv2D(32, kernel_size=(3, 1), activation="relu", input_shape=input_shape))
model.add(layers.MaxPooling2D(pool_size=(2, 1)))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation="relu"))
model.add(layers.Dense(10, activation="softmax"))

model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=["accuracy"])
fitted = model.fit(data_train, labels_train_categorical, batch_size=128, epochs=15, verbose=1, validation_data=(data_test, labels_test_categorical))

score = model.evaluate(data_test, labels_test_categorical, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

#CNN trial 2 - deeper net
input_shape = (40, 1, 1)

labels_train_categorical = keras.utils.to_categorical(labels_train)
labels_test_categorical = keras.utils.to_categorical(labels_test)

def CNN_256():
    input_shape = (40, 1, 1)

    model = keras.Sequential()
    model.add(layers.Conv2D(32, kernel_size=(3, 1), activation="relu", input_shape=input_shape))
    model.add(layers.MaxPooling2D(pool_size=(2, 1)))
    model.add(layers.Conv2D(64, kernel_size=(3, 1), activation="relu"))
    model.add(layers.MaxPooling2D(pool_size=(2, 1)))
    model.add(layers.Conv2D(128, kernel_size=(3, 1), activation="relu"))
    model.add(layers.MaxPooling2D(pool_size=(2, 1)))
    model.add(layers.Conv2D(256, kernel_size=(3, 1), activation="relu"))
    model.add(layers.MaxPooling2D(pool_size=(1, 1)))
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation="relu"))
    model.add(layers.Dense(10, activation="softmax"))

    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=["accuracy"])
    return model

model = CNN_256()

#model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=["accuracy"])

fitted2 = model.fit(data_train, labels_train_categorical, batch_size=128, epochs=20, verbose=1, validation_data=(data_test, labels_test_categorical))

score2 = model.evaluate(data_test, labels_test_categorical, verbose=0)
print("Test loss:", score2[0])
print("Test accuracy:", score2[1])

print(labels_train_categorical.shape)
print(labels_test_categorical.shape)

history = fitted2 #model.fit(data_train, labels_train_categorical, batch_size=128, epochs=10, verbose=1, validation_data=(data_test, labels_test_categorical))
history

import matplotlib.pyplot as plt

# Get the training history
#history = model.historyhistory = model.fit(data_train, labels_train_categorical, batch_size=128, epochs=10, verbose=1, validation_data=(data_test, labels_test_categorical))

# Extract the training and validation loss values
train_loss = history.history['loss']
val_loss = history.history['val_loss']

# Extract the training and validation accuracy values
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

fig, axs = plt.subplots(1, 2, figsize=(12, 6))

axs[0].plot(range(1, len(train_loss) + 1), train_loss, 'b-', label='Training Loss')
axs[0].plot(range(1, len(val_loss) + 1), val_loss, 'r-', label='Validation Loss')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].set_title('Training and Validation Loss')
axs[0].legend()

axs[1].plot(range(1, len(train_accuracy) + 1), train_accuracy, 'b-', label='Training Accuracy')
axs[1].plot(range(1, len(val_accuracy) + 1), val_accuracy, 'r-', label='Validation Accuracy')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].set_title('Training and Validation Accuracy')
axs[1].legend()

plt.tight_layout()
plt.show()

#tune hyperparameters
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.callbacks import LambdaCallback

def print_progress(epoch, logs):
    if epoch % 5 == 0:
        print(f"Epoch {epoch}/{grid.estimator.epochs}, GridSearchCV Score: {grid_result.best_score_:.4f}")

# Define the model
def CNN_256_v2(filters=32, kernel_size=(3, 1), learning_rate=0.001):
    input_shape = (40, 1, 1)

    model = keras.Sequential()
    model.add(layers.Conv2D(32, kernel_size=(3, 1), activation="relu", input_shape=input_shape))
    model.add(layers.MaxPooling2D(pool_size=(2, 1)))
    model.add(layers.Conv2D(64, kernel_size=(3, 1), activation="relu"))
    model.add(layers.MaxPooling2D(pool_size=(2, 1)))
    model.add(layers.Conv2D(128, kernel_size=(3, 1), activation="relu"))
    model.add(layers.MaxPooling2D(pool_size=(2, 1)))
    model.add(layers.Conv2D(256, kernel_size=(3, 1), activation="relu"))
    model.add(layers.MaxPooling2D(pool_size=(1, 1)))
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation="relu"))
    model.add(layers.Dense(10, activation="softmax"))

    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=["accuracy"])

    return model

# Define the parameter grid for grid search
param_grid = {
    'filters': [32, 64, 128, 256],
    'kernel_size': [(3, 1), (5, 1)],
    'learning_rate': [0.001, 0.01, 0.1]
}

# Create the model
model = keras.wrappers.scikit_learn.KerasClassifier(build_fn=CNN_256_v2, epochs=15, batch_size=128, verbose=1) #can use create_model

# Perform grid search with cross-validation
grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)
#callback
#progress_callback = LambdaCallback(on_epoch_end=lambda epoch, logs: print_progress(epoch, logs, grid.estimator.epochs))
progress_callback = LambdaCallback(on_epoch_end=print_progress)
#train model and track progress
grid_result = grid.fit(data_train, labels_train_categorical)
#grid_result = grid.fit(data_train, labels_train_categorical, callbacks=[progress_callback])

# Print the best hyperparameters and score
print("Best Score:", grid_result.best_score_)
print("Best Parameters:")
best_params = grid_result.best_params_
for param, value in best_params.items():
    print(f"{param}: {value}")

#CNN update hyperparameters from GridSearchCV
#Best Score: 0.6552132566769918
#Best Parameters:
#filters: 256
#kernel_size: (5, 1)
#learning_rate: 0.001

input_shape = (40, 1, 1)

labels_train_categorical = keras.utils.to_categorical(labels_train)
labels_test_categorical = keras.utils.to_categorical(labels_test)

def CNN_256_upgrade():
    input_shape = (40, 1, 1)

    model = keras.Sequential()
    model.add(layers.Conv2D(256, kernel_size=(5, 1), activation="relu", padding="same", input_shape=input_shape))
    model.add(layers.MaxPooling2D(pool_size=(2, 1)))
    model.add(layers.Conv2D(256, kernel_size=(5, 1), activation="relu", padding="same"))
    model.add(layers.MaxPooling2D(pool_size=(2, 1)))
    model.add(layers.Conv2D(256, kernel_size=(5, 1), activation="relu", padding="same"))
    model.add(layers.MaxPooling2D(pool_size=(2, 1)))
    model.add(layers.Conv2D(256, kernel_size=(5, 1), activation="relu", padding="same"))
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dense(64, activation="relu"))
    model.add(layers.Dense(10, activation="softmax"))

    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=["accuracy"])
    return model
model = CNN_256_upgrade()

fitted3 = model.fit(data_train, labels_train_categorical, batch_size=128, epochs=20, verbose=1, validation_data=(data_test, labels_test_categorical))

score3 = model.evaluate(data_test, labels_test_categorical, verbose=0)
print("Test loss:", score3[0])
print("Test accuracy:", score3[1])
#Test loss: 1.2262206077575684
#Test accuracy: 0.7387776970863342

"""BUILDING MACHINE LEARNING PIPELINE"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.base import BaseEstimator, TransformerMixin

#uisng preprocessing function from above ^^^

# Convert labels to categorical format
labels_train_categorical = keras.utils.to_categorical(labels_train)
labels_test_categorical = keras.utils.to_categorical(labels_test)

# Using deep layer CNN model architecture as a function
#not using this one, using create_model from above ^^^
#def create_model2(filters=32, kernel_size=(3, 1), learning_rate=0.001):
   # model = keras.Sequential()
   # model.add(layers.Conv2D(32, kernel_size=(3, 1), activation="relu", input_shape=(40, 1, 1)))
   # model.add(layers.MaxPooling2D(pool_size=(2, 1)))
   # model.add(layers.Conv2D(64, kernel_size=(3, 1), activation="relu"))
   # model.add(layers.MaxPooling2D(pool_size=(2, 1)))
   # model.add(layers.Conv2D(128, kernel_size=(3, 1), activation="relu"))
   # model.add(layers.MaxPooling2D(pool_size=(2, 1)))
   # model.add(layers.Conv2D(256, kernel_size=(3, 1), activation="relu"))
   # model.add(layers.MaxPooling2D(pool_size=(1, 1)))
   # model.add(layers.Flatten())
   # model.add(layers.Dense(64, activation="relu"))
   # model.add(layers.Dense(10, activation="softmax"))
  #  optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
 #   model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=["accuracy"])
 #   return model

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data_train, labels_train_categorical, test_size=0.2, random_state=42)

#custom reshape standardscaler to ajust input dimensions

class ReshapeScaler(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.mean = None
        self.std = None

    def fit(self, X, y=None):
        # Reshape the data to (num_samples, num_features, 1, 1)
        X_reshaped = X.reshape(X.shape[0], X.shape[1], 1, 1)
        # Compute mean and standard deviation
        self.mean = np.mean(X_reshaped, axis=(0, 1))
        self.std = np.std(X_reshaped, axis=(0, 1))
        return self

    def transform(self, X):
        # Reshape the data to (num_samples, num_features, 1, 1)
        X_reshaped = X.reshape(X.shape[0], X.shape[1], 1, 1)
        # Scale the data
        X_scaled = (X_reshaped - self.mean) / self.std
        return X_scaled

# Create the pipeline
pipeline = Pipeline([
    ('reshape_scaling', ReshapeScaler()), #reshape and scale
   # ('scaling', StandardScaler()), # Preprocessing step: feature scaling
    ('classification', KerasClassifier(build_fn=CNN_256_upgrade, epochs=20, batch_size=128, verbose=1))
])

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

# Evaluate the pipeline on the validation data
score = pipeline.score(X_val, y_val)
print("Validation accuracy:", score)

# Predict labels for the test data
y_pred = pipeline.predict(data_test)
accuracy = accuracy_score(labels_test_categorical.argmax(axis=1), y_pred)
print("Test accuracy:", accuracy)